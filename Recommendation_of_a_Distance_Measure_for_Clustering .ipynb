{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehwjxriMkG0H"
      },
      "source": [
        "### 寫成函式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaP8Fs1hz4Wu",
        "outputId": "cad58ffb-0c76-406c-91ee-bf2968089893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyclustering in /usr/local/lib/python3.7/dist-packages (0.10.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (7.1.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pyclustering) (1.7.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->pyclustering) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->pyclustering) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->pyclustering) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apyori in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#colab環境設定\n",
        "!pip3 install pyclustering\n",
        "!pip install apyori\n",
        "\n",
        "from google.colab import drive\n",
        "from pyclustering.cluster.kmeans import kmeans, kmeans_visualizer\n",
        "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
        "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
        "from pyclustering.utils.metric import distance_metric, type_metric\n",
        "from pyclustering.utils import read_sample\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.stats import skew,norm, kurtosis\n",
        "from scipy.spatial.distance import cdist\n",
        "from time import time\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from apyori import apriori\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy\n",
        "import statistics\n",
        "import csv\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X61VGgs7DpGd"
      },
      "outputs": [],
      "source": [
        "def Category_one(file_name):\n",
        "\tdata = pd.read_csv(file_name, index_col=0)\n",
        "\tdf = pd.DataFrame(data)\n",
        "\tfeatures_arr = np.empty(shape=4,dtype=float)\n",
        "\tNum_instances, Num_features = df.shape#Number of instances,Number of features\n",
        "\t#for x in range(int(Num_instances/50),Num_instances):#減少資料量\n",
        "\t#\tdf=df.drop(x, axis = 0)\n",
        "\t#print(df.dtypes)\n",
        "\tData_dim=Num_instances/Num_features#Data dimensionality\n",
        "\t#print(\"Num_instances=\",Num_instances,\"Num_features=\",Num_features,\"Data_dim=\",Data_dim)\n",
        "\tfeatures_arr[0]=Num_instances\n",
        "\tfeatures_arr[1]=Num_features\n",
        "\tfeatures_arr[2]=Data_dim\n",
        "\tentropy_arr = np.zeros(shape=(Num_features-1))\n",
        "\ti=0;\n",
        "\tfor col in df.drop(columns='class'):\n",
        "\t\tprobability = df[col].value_counts(normalize=True)\n",
        "\t\tentropy=-1*np.sum(np.log2(probability)*probability)\n",
        "\t\tentropy_arr[i]=entropy\n",
        "\t\ti=i+1\n",
        "\tmean_entropy=entropy_arr.mean()\n",
        "\t#print(\"mean_entropy\",mean_entropy)#mean entropy of all attributes\n",
        "\tfeatures_arr[3]=mean_entropy\n",
        "\n",
        "\treturn features_arr\n",
        "\n",
        "def isInList(aList, item):\n",
        "    if item in aList:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Function to remove strings (e.g., white space) from the list items\n",
        "\n",
        "\n",
        "def removeEmptyValues(aList, symbolToRemove):\n",
        "    while (True):\n",
        "        val = isInList(aList, symbolToRemove)\n",
        "        if (val == 1):\n",
        "            aList.remove(symbolToRemove)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "# Function to create a list of lists from a CSV file containing transaction data\n",
        "\n",
        "\n",
        "def deletecol(data, i):\n",
        "    for row in data:\n",
        "        del row[i]\n",
        "    return data\n",
        "\n",
        "# Function to create a list of lists from a CSV file containing transaction data\n",
        "\n",
        "\n",
        "def generateListOfListsFromCSV(fullFileName):\n",
        "\n",
        "    with open(fullFileName, newline='', encoding='utf-8-sig') as f:\n",
        "        reader = csv.reader(f, delimiter=\",\")\n",
        "        data = list(reader)\n",
        "\n",
        "    listOfLists = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        removeEmptyValues(data[i], '')\n",
        "        listOfLists.append(data[i])\n",
        "\n",
        "    return listOfLists\n",
        "\n",
        "# Function to create a one-hot encoded DataFrame object from the Python list of lists\n",
        "\n",
        "\n",
        "def oneHotEncodedDataFrame(data):\n",
        "\n",
        "    # Transform the transaction data into a one-hot encoded NumPy boolean array\n",
        "    te = TransactionEncoder()\n",
        "    te_ary = te.fit(data).transform(data)\n",
        "\n",
        "    # Read the one-hot encoded data as a Pandas DataFrame object\n",
        "    dataFrame = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "    return dataFrame\n",
        "\n",
        "# Function to create a list of items & baskets from a one-hot encoded DataFrame object\n",
        "\n",
        "\n",
        "def generateItemsBaskets(dataFrame):\n",
        "    # Create a list of items from the DataFrame object\n",
        "    items = []\n",
        "    for col in dataFrame.columns:\n",
        "        items.append(col)\n",
        "\n",
        "    # Create a list of baskets from the DataFrame object\n",
        "    baskets = []\n",
        "    transaction_id = 1\n",
        "\n",
        "    for i in range(len(dataFrame)):\n",
        "        itemset = []\n",
        "        for j in range(len(items)):\n",
        "            if (dataFrame.iloc[i][j] == True):\n",
        "                itemset.append(items[j])\n",
        "        baskets.append({transaction_id: itemset})\n",
        "        transaction_id += 1\n",
        "\n",
        "    return items, baskets\n",
        "\n",
        "\n",
        "def bruteForceFrequentItemsets(dataFrame, min_support=2, max_len=None):\n",
        "\n",
        "    # Create a list of items & baskets from a one-hot encoded DataFrame object\n",
        "    items, baskets = generateItemsBaskets(dataFrame)\n",
        "\n",
        "    # Declare a list to store frequent 2-itemsets\n",
        "    freq_itemsets = []\n",
        "\n",
        "    for i in range(len(items)):\n",
        "\n",
        "        for j in range(len(items)):\n",
        "\n",
        "            if (j != i and j > i):\n",
        "                basket_count = 0\n",
        "                basket_item = 1\n",
        "                for b in baskets:\n",
        "                    val1 = isInList(b.get(basket_item), items[i])\n",
        "                    val2 = isInList(b.get(basket_item), items[j])\n",
        "                    if (val1 != -1 and val2 != -1):\n",
        "                        basket_count += 1\n",
        "                    basket_item += 1\n",
        "\n",
        "                if (basket_count >= min_support):\n",
        "                    freq_itemsets.append([items[i], items[j]])\n",
        "\n",
        "    return freq_itemsets\n",
        "\n",
        "def Category_two(file_name):\n",
        "\titemSetList = generateListOfListsFromCSV(file_name)\n",
        "\t# print(itemSetList)\n",
        "\titemSetList.pop(0)\n",
        "\t#print(\"=======================================================================================================================================\")\n",
        "\n",
        "\titemSetList = deletecol(itemSetList, (len(itemSetList[0])-1))    #若CSV中有column要刪除就用這個,刪除column class\n",
        "\t#print(oneHotEncodedDataFrame(itemSetList))\n",
        "\t# print(freqItemSet)\n",
        "\n",
        "\tassociation_rules = apriori(\n",
        "\t\titemSetList, min_support=0.05, min_confidence=0, min_lift=1, min_length=1)\n",
        "\tassociation_results = list(association_rules)\n",
        "\t# items = [x for x in pair]\n",
        "\t# print(\"Support: \" + str(item[1]))\n",
        "\tfeatures_arr = []\n",
        "\t#print(\"1-itemset :\")\n",
        "\tone_itemset_arr = []\n",
        "\tfor i in range(0, len(association_results)):\n",
        "\t\tif (len(association_results[i][0]) == 1):\n",
        "\t\t\t#print(association_results[i][0],\n",
        "\t\t\t#\t\"Support : \", association_results[i][1])\n",
        "\t\t\tone_itemset_arr.append(association_results[i][1])\n",
        "\tone_itemset_arr = np.array(one_itemset_arr)\n",
        "\tone_itemset_arr_sort = np.sort(one_itemset_arr)#sorted in ascending order according\n",
        "\tone_itemset_arr_size=len(one_itemset_arr_sort)\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[0])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8*2))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8*3))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8*4))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8*5))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8*6))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[int((one_itemset_arr_size/8*7))])\n",
        "\tfeatures_arr.append(one_itemset_arr_sort[one_itemset_arr_size-1])\n",
        "\n",
        "\ttwo_itemset_arr = []\n",
        "\t#print(\"2-itemset :\")\n",
        "\tfor i in range(0, len(association_results)):\n",
        "\t\tif (len(association_results[i][0]) == 2):\n",
        "\t\t\t#print(association_results[i][0],\n",
        "\t\t\t#\t\"Support : \", association_results[i][1])\n",
        "\t\t\ttwo_itemset_arr.append(association_results[i][1])\n",
        "\ttwo_itemset_arr = np.array(two_itemset_arr)\n",
        "\ttwo_itemset_arr_sort = np.sort(two_itemset_arr)#sorted in ascending order according\n",
        "\ttwo_itemset_arr_size=len(two_itemset_arr_sort)\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[0])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8*2))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8*3))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8*4))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8*5))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8*6))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[int((two_itemset_arr_size/8*7))])\n",
        "\tfeatures_arr.append(two_itemset_arr_sort[two_itemset_arr_size-1])\n",
        "\t#print(\"features_arr=\",features_arr)\n",
        "\n",
        "\treturn features_arr\n",
        "\n",
        "\n",
        "#Function to  create % of values of S1 in the interval [0,0.1], . . . , (0.9,1.0]\n",
        "\n",
        "def create_S1_fre_arr(S):\n",
        "\tmin_S=min(S)#minimum distance values in S\n",
        "\tmax_S=max(S)#maximum distance values in S\n",
        "\tmax_min_diff_distance=max_S-min_S\n",
        "\tS_size=len(S)\n",
        "\tS_1 = np.zeros(shape=S_size)\n",
        "\tS_1_fre = np.zeros(shape=10)#The frequencies of distance values in each interval of S\n",
        "\tfor i in range(len(S)-1):\n",
        "\t\tS_1[i]=(S[i]-min_S)/max_min_diff_distance\n",
        "\t\tif S_1[i]<0.01:\n",
        "\t\t\tS_1_fre[0]=S_1_fre[0]+1\n",
        "\t\telif S_1[i]<0.02:\n",
        "\t\t\tS_1_fre[1]=S_1_fre[1]+1\n",
        "\t\telif S_1[i]<0.03:\n",
        "\t\t\tS_1_fre[2]=S_1_fre[2]+1\n",
        "\t\telif S_1[i]<0.04:\n",
        "\t\t\tS_1_fre[3]=S_1_fre[3]+1\n",
        "\t\telif S_1[i]<0.05:\n",
        "\t\t\tS_1_fre[4]=S_1_fre[4]+1\n",
        "\t\telif S_1[i]<0.06:\n",
        "\t\t\tS_1_fre[5]=S_1_fre[5]+1\n",
        "\t\telif S_1[i]<0.07:\n",
        "\t\t\tS_1_fre[6]=S_1_fre[6]+1\n",
        "\t\telif S_1[i]<0.08:\n",
        "\t\t\tS_1_fre[7]=S_1_fre[7]+1\n",
        "\t\telif S_1[i]<0.09:\n",
        "\t\t\tS_1_fre[8]=S_1_fre[8]+1\n",
        "\t\telse :\n",
        "\t\t\tS_1_fre[9]=S_1_fre[9]+1\n",
        "\t\n",
        "\tS_1_fre=S_1_fre/S_size\n",
        "\t#print(\"S_1_fre=\",S_1_fre)#debug\n",
        "\treturn S_1_fre\n",
        "\n",
        "#Function to  create % of values of S2 in the interval [0,1), . . . , [3,+∞)\n",
        "\n",
        "def create_S2_fre_arr(S,Mean_S,Standard_deviation_S):\n",
        "\tS_2 = np.zeros(shape=len(S))#Each distance value s ∈ S is normalized\n",
        "\tS_2_fre = np.zeros(shape=4)#The frequencies of distance values in each interval are computed\n",
        "\tS_size=len(S)\n",
        "\tfor i in range(S_size-1):\n",
        "\t\tS_2[i]=(S[i]-Mean_S)/Standard_deviation_S\n",
        "\t\tif S_2[i]<1:\n",
        "\t\t\tS_2_fre[0]=S_2_fre[0]+1\n",
        "\t\telif S_2[i]<2:\n",
        "\t\t\tS_2_fre[1]=S_2_fre[1]+1\n",
        "\t\telif S_2[i]<3:\n",
        "\t\t\tS_2_fre[2]=S_2_fre[2]+1\n",
        "\t\telse :\n",
        "\t\t\tS_2_fre[3]=S_2_fre[3]+1\n",
        "\tS_2_fre=S_2_fre/S_size\n",
        "\t#print(\"S_2_fre=\",S_2_fre)#debug\n",
        "\treturn S_2_fre\n",
        "\n",
        "def Category_three(file_name,Num_instances):\n",
        "\tdataset = generateListOfListsFromCSV(file_name)\n",
        "\tdataset_onehotencoded = oneHotEncodedDataFrame(dataset)\n",
        "\tdataset_onehotencoded = dataset_onehotencoded.astype(int)\n",
        "\tNum_instances_int=Num_instances.astype(int)\n",
        "\tS_size=(Num_instances_int-1)*Num_instances_int/2\n",
        "\tS = np.zeros(shape=int(S_size))#The Euclidean distance values between all pairs of the data objects in dataset\n",
        "\ti=0\n",
        "\tfor x in range(1,Num_instances_int):\n",
        "\t\tfor y in range(x+1,Num_instances_int):\n",
        "\t\t\tS[i] = numpy.linalg.norm(dataset_onehotencoded.loc[x]-dataset_onehotencoded.loc[y])\n",
        "\t\t\ti=i+1\n",
        "\tfeatures_arr = np.empty(5,dtype=float)\n",
        "\tMean_S=S.mean()#Mean of S\n",
        "\t#print(\"Mean_S=\",Mean_S)#debug\n",
        "\tfeatures_arr[0]=Mean_S\n",
        "\tVariance_S=statistics.variance(S)#Variance of S\n",
        "\t#print(\"Variance_S=\",Variance_S)#debug\n",
        "\tfeatures_arr[1]=Variance_S\n",
        "\tStandard_deviation_S = numpy.std(S)#Standard deviation of S\n",
        "\t#print(\"Standard_deviation_S=\",Standard_deviation_S)#debug\n",
        "\tfeatures_arr[2]=Standard_deviation_S\n",
        "\tSkew_S=skew(S)#Skewness of S\n",
        "\t#print(\"Skew_S=\",Skew_S)#debug\n",
        "\tfeatures_arr[3]=Skew_S\n",
        "\tKurtosis_S=kurtosis(S)#Kurtosis of S\n",
        "\t#print(\"Kurtosis_S=\",Kurtosis_S)#debug\n",
        "\tfeatures_arr[4]=Kurtosis_S\n",
        "\tS1_fre_arr=create_S1_fre_arr(S)\n",
        "\tfeatures_arr=np.hstack([features_arr,S1_fre_arr])\n",
        "\tS2_fre_arr=create_S2_fre_arr(S,Mean_S,Standard_deviation_S)\n",
        "\tfeatures_arr=np.hstack([features_arr,S2_fre_arr])\n",
        "\n",
        "\treturn features_arr\n",
        "\n",
        "\n",
        "def metaFeatureCollection(file_name):\n",
        "\n",
        "\tfeatures_arr=np.empty(4,dtype=float);\n",
        "\tSta_info_arr = Category_one(file_name)#Statistical and information theory-based\n",
        "\t#print(\"Sta_info_arr=\",Sta_info_arr)\n",
        "\tfeatures_arr = Sta_info_arr\n",
        "\t#Category_two(file_name)\n",
        "\tStruc_info_arr=Category_two(file_name)#Structural information-based\n",
        "\tfeatures_arr=np.concatenate([features_arr,Struc_info_arr])\n",
        "\tDis_info_arr = Category_three(file_name,features_arr[0])#Distance information-based\n",
        "\t#print(\"Dis_info_arr=\",Dis_info_arr)\n",
        "\tfeatures_arr=np.concatenate([features_arr,Dis_info_arr])\n",
        "\t#print(\"features_arr=\",features_arr)\n",
        "\n",
        "\treturn features_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4mRUuaikG0K"
      },
      "outputs": [],
      "source": [
        "def euclidean_dis(x, y):\n",
        "    return np.sqrt(np.sum((x - y) ** 2))\n",
        "def manhattan_dis(x, y):\n",
        "    return np.sum(np.abs(x - y))\n",
        "def StandardizedEuclidean(x, y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X = np.vstack([x, y])\n",
        "    sigma = np.var(X, axis=0, ddof=1)\n",
        "    # print(sigma)\n",
        "    return np.sqrt(((x - y) ** 2 / sigma).sum())\n",
        "def cosin_dis(x, y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return 1 - np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "    \n",
        "# metrix = distance_metrix(type_metrix.USER_DEFINED, func=euclidean_dis)\n",
        "\n",
        "def get_dismetrix(dis_method):\n",
        "    if dis_method == 'euclidean':\n",
        "        metric = distance_metric(type_metric.EUCLIDEAN)\n",
        "    elif dis_method == 'manhattan':\n",
        "        metric = distance_metric(type_metric.MANHATTAN)\n",
        "    elif dis_method == 'chebyshev':\n",
        "        metric = distance_metric(type_metric.CHEBYSHEV)\n",
        "    elif dis_method == 'minkowski':\n",
        "        metric = distance_metric(type_metric.MINKOWSKI)\n",
        "    elif dis_method == 'canberra':\n",
        "        metric = distance_metric(type_metric.CANBERRA)\n",
        "    elif dis_method == 'stand_eulidean':\n",
        "        metric = distance_metric(type_metric.USER_DEFINED, func=StandardizedEuclidean)\n",
        "    elif dis_method == 'cosin_dis':\n",
        "        metric = distance_metric(type_metric.USER_DEFINED, func=cosin_dis)\n",
        "    return metric\n",
        "\n",
        "def get_cluster_object(Data, initial_centers, metric, algo):\n",
        "    if algo == 'kmeans':\n",
        "        kmeans_instance = kmeans(Data, initial_centers, metric=metric)\n",
        "        return kmeans_instance\n",
        "def BuildCluster(Data, dis_method, algo, n_clusters):\n",
        "\n",
        "    initial_centers = kmeans_plusplus_initializer(Data, n_clusters).initialize()\n",
        "    initial_centers = np.array(initial_centers)\n",
        "\n",
        "    # Create instance of K-Means algorithm with prepared centers.\n",
        "    metric = get_dismetrix(dis_method)\n",
        "    kmeans_instance  = get_cluster_object(Data, initial_centers, metric, algo)\n",
        "    # kmeans_instance = kmeans(Data, initial_centers, metric=metric)\n",
        "    # Run cluster analysis and obtain results.\n",
        "    kmeans_instance.process()\n",
        "    clusters = kmeans_instance.get_clusters()\n",
        "    return clusters\n",
        "def evaluate(clusters, cli, E):\n",
        "    score = 0\n",
        "    if E == 'f-score':\n",
        "        y_pred = [0 for i in range(len(cli))]\n",
        "        for j in range(len(clusters)):\n",
        "            for i in range(len(clusters[j])):\n",
        "                y_pred[clusters[j][i]] = cli[clusters[j]].value_counts().index[0]\n",
        "        score = f1_score(cli, y_pred, average='macro')\n",
        "    return score\n",
        "def Meta_target_identification(D = [], S = ['euclidean', 'manhattan'],\n",
        "                   E = 'f-score', C = 'kmeans', F = 'single'):\n",
        "    Evalmetric = []\n",
        "    DSRank = []\n",
        "    Y = []\n",
        "    for i in range(len(D)):\n",
        "        cli = D[i]['class']\n",
        "        D[i] = D[i].drop(['class'], axis=1) \n",
        "        n_clusters = len(cli.value_counts())\n",
        "        for j in range(len(S)):\n",
        "            Clusters = BuildCluster(D[i], S[j], C, n_clusters)\n",
        "            value = evaluate(Clusters, cli, E)\n",
        "            Evalmetric.append(value)\n",
        "        # dsrank is sorting evalmetric descending return index\n",
        "        DSRank = np.argsort(Evalmetric)[::-1]\n",
        "        #print(DSRank)\n",
        "        if F == 'single':\n",
        "            optimal_index = DSRank[0]\n",
        "            optimal_dis_measure = S[optimal_index]\n",
        "            Y.append([optimal_dis_measure])\n",
        "        else:\n",
        "            Y.append(DSRank)\n",
        "        Evalmetric.clear()\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kses8ex1HG6C"
      },
      "outputs": [],
      "source": [
        "def EuclideanDist (Xnew, neighbor) :\n",
        "  temp = 0\n",
        "  for i in range(len(Xnew[0])):\n",
        "    temp += (Xnew[0][i] - neighbor[i])**2\n",
        "  return (temp**(1/2))\n",
        "\n",
        "def NearNeighbors(M, Xnew, k, q):\n",
        "  neigh = NearestNeighbors(n_neighbors=k)\n",
        "  neigh.fit(M[0])\n",
        "  Neighbors = neigh.kneighbors(Xnew)\n",
        "\n",
        "  #算出每個neighbor到Xnew的距離，並將其倒數設為權重\n",
        "  weight = [0] * k\n",
        "  for i in range(k) : \n",
        "    #Neighbors[1][0][i]為第i個最接近Xnew的neighbor index\n",
        "    #neighbor為由第i個最接近Xnew的neighbor中的Training dataset，所萃取出來的Data\n",
        "    neighbor = M[0][Neighbors[1][0][i]]\n",
        "    d = EuclideanDist(Xnew, neighbor)\n",
        "    if d!=0 :\n",
        "      weight[i] = 1/d\n",
        "    else : \n",
        "      #將無限大改為1\n",
        "      weight[i] = 1\n",
        "\n",
        "  #算出每個Measure對應於其他neighbors的權重值\n",
        "  scores = [0] * q\n",
        "  for i in range(q):\n",
        "    for j in range(k):\n",
        "      #Neighbors[1][0][j]為距離Xnew第j近的neighbor index\n",
        "      #y為此index所對應到的measures list\n",
        "      y = M[1][Neighbors[1][0][j]]\n",
        "      #找出第i個Measure在此list中的index，以此作為其重要性\n",
        "      ind = y.index(i)\n",
        "      scores[i] += weight[j] * ( q - ind )\n",
        "  return np.argsort(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JZw1X02kG0M",
        "outputId": "778f1f4d-750e-4722-be35-925f8e89c28d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2 3 5 4]\n"
          ]
        }
      ],
      "source": [
        "#training DataSet that was inputted have to name meta-target as \"class\"\n",
        "#data_path need to specified in advance.\n",
        "# encode feature\n",
        "\n",
        "S = ['euclidean', 'manhattan', 'chebyshev','canberra', 'stand_eulidean', 'cosin_dis']# , 'manhattan'\n",
        "E = ['f-score']\n",
        "C = ['kmeans']\n",
        "F = ['ranking'] # 'single', \n",
        "data_path = ['/content/drive/MyDrive/Colab_Notebooks/lowbwt.csv','/content/drive/MyDrive/Colab_Notebooks/dataset_13_breast-cancer.csv',\n",
        "                '/content/drive/MyDrive/Colab_Notebooks/dataset_10_lymph.csv','/content/drive/MyDrive/Colab_Notebooks/cholesterol.csv',\n",
        "             '/content/drive/MyDrive/Colab_Notebooks/primary-tumor.csv','/content/drive/MyDrive/Colab_Notebooks/mushrooms.csv',\n",
        "             '/content/drive/MyDrive/Colab_Notebooks/DowJones.csv','/content/drive/MyDrive/Colab_Notebooks/diabetes.csv',\n",
        "             '/content/drive/MyDrive/Colab_Notebooks/car_evaluation.csv']\n",
        "\n",
        "Database = []\n",
        "for i in range(len(data_path)):\n",
        "  Data = pd.read_csv(data_path[i])\n",
        "  for j in range(len(Data.columns)):\n",
        "    Data[Data.columns[j]]= Data[Data.columns[j]].astype('category')\n",
        "    Data[Data.columns[j]] = Data[Data.columns[j]].cat.codes\n",
        "  Database.append(Data)\n",
        "\n",
        "Y = Meta_target_identification(D = Database, S = S, E = E[0], C = C[0], F = F[0])\n",
        "X = []\n",
        "M = [X, Y]\n",
        "q = len(S)\n",
        "# 自定義參數K，代表欲選擇Neighbors的數量\n",
        "k = 2\n",
        "#挑選一個不再Training data中的dataset\n",
        "Xnew = metaFeatureCollection('/content/drive/MyDrive/Colab_Notebooks/heart2.csv')\n",
        "\n",
        "#將Database的每個dataset extracte後加到X中\n",
        "for i in range(len(data_path)):\n",
        "  X.append(metaFeatureCollection(data_path[i]))\n",
        "#將X的參數型態改成list\n",
        "for i in range(len(M[0])) : \n",
        "  M[0][i] = M[0][i].tolist()\n",
        "#將Y的參數型態改成list\n",
        "for i in range(len(M[1])) : \n",
        "  M[1][i] = M[1][i].tolist()\n",
        "\n",
        "#由於Kneighbor不支援多個target value，因此先將Y做等級分群\n",
        "Y_component = []\n",
        "for j in range(len(M[1][i])) :\n",
        "  temp = []\n",
        "  for i in range(len(M[1])) :\n",
        "    temp.append(M[1][i][j])\n",
        "  Y_component.append(temp)\n",
        "\n",
        "ynew = NearNeighbors(M, [Xnew], k, q)\n",
        "print(ynew)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5850f293d5712e5c6d19cea0f56365f6e8795abb17dc044d20844fbb0bf63f46"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
